# import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from feature_engine.outliers import Winsorizer
from sklearn.model_selection import train_test_split
#import dtale
import pickle, joblib
from sqlalchemy import create_engine, text
from urllib.parse import quote

# Creating engine which connect to MySQL
# Add Mysql Database credentials
'''
user = 'user1' # user name
pw = '360@Digi#TMG' # password
db = 'amerdb' # database
'''
'''
user = 'root' # user name
pw = 'Classroom' # password
db = 'Abhimadhu1' # database
# creating engine to connect database
engine = create_engine(f"mysql+pymysql://{user}:%s@localhost/{db}" %quote(f'{pw}'))
'''
#lets import the data
data = pd.read_csv(r"prima_13.csv")

# dumping data into database 
#data.to_sql('prima_13', con = engine, if_exists = 'replace', chunksize = 1000, index = False)

user_name = 'root'
database = 'Classroom'
your_password = 'Abhimadhu1'
engine = create_engine(f'mysql+pymysql://{user_name}:%s@localhost:3306/{database}' % quote(f'{your_password}'))

data.to_sql('prima_13', con = engine, if_exists = 'replace', chunksize = 1000, index = False)
# loading data from database
sql = text('select * from prima_13')
df = pd.read_sql_query(sql, con = engine.connect())

print(df)

df.info()
'''
# autoeda
dtale.show(df).open_browser()
'''
#Descriptive Analytics
desc = df.describe()

# Datapreprocessing by Pipeline
df.columns
# Seperating input and output variables 
df_x = df.drop(['Date', 'Machine_ID', 'Downtime'], axis = 1)
df_y = df[['Downtime']]

# All numeric features
numeric_features = df_x.select_dtypes(exclude = ['object']).columns
numeric_features

# All categorical features
categorical_features = df_x.select_dtypes(include = ['object']).columns
categorical_features# No Categorical features in predictors

# Imputation strategy for numeric columns
num_pipeline = Pipeline([('impute', SimpleImputer(strategy = 'mean')),('scale', MinMaxScaler())])
                                                  
# Using ColumnTransfer to transform the columns of an array or pandas DataFrame. This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space.
preprocess_pipeline = ColumnTransformer([('numerical', num_pipeline, numeric_features)])

processed = preprocess_pipeline.fit(df_x)  # Pass the raw data through pipeline

processed

# Save the defined pipeline
joblib.dump(processed, 'imp_scale')

import os 
os.getcwd()

# Transform the original data using the pipeline defined above
cleandata = pd.DataFrame(processed.transform(df_x), columns = processed.get_feature_names_out())  # Clean and processed data for Clustering

cleandata.info()

# columns list
columns_list = list(cleandata.columns)
columns_list

########## Boxplot before cleaning
ax = cleandata.plot(kind='box', subplots=True, sharey=False, figsize=(40, 18))
# Set x-axis labels rotation and fontsize for each subplot
for subplot in ax:
    subplot.set_xticklabels(subplot.get_xticklabels(), rotation=90, fontsize=21)
# Adjust the spacing between subplots
plt.subplots_adjust(wspace=1.0)
# Show the plot
plt.show()

##########
winsor = Winsorizer(capping_method = 'iqr', # choose  IQR rule boundaries or gaussian for mean and std
                          tail = 'both', # cap left, right or both tails 
                          fold = 1.5,
                          variables = columns_list)

# Fit the winsorizer to numerical columns
outlier = winsor.fit(cleandata[columns_list])
# Save the fitted winsorizer model 
joblib.dump(outlier, 'winsor')
cleandata[columns_list] = outlier.transform(cleandata[columns_list])

########## Boxplot after cleaning
ax = cleandata.plot(kind='box', subplots=True, sharey=False, figsize=(40, 18))
# Set x-axis labels rotation and fontsize for each subplot
for subplot in ax:
    subplot.set_xticklabels(subplot.get_xticklabels(), rotation=90, fontsize=21)
# Adjust the spacing between subplots
plt.subplots_adjust(wspace=1.0)
# Show the plot
plt.show()

### check for balanced or imbalanced data
df_y['Downtime'].value_counts()

# The ratio of NON_FAILURE to FAILURE can be calculated as:
# 2400 / 300 = 8
# This means that there are 8 times more instances of NON_FAILURE than FAILURE in dataset.
# When the ratio of instances between the classes is significantly imbalanced, as in this case, 
# the data is considered imbalanced.

from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(cleandata, df_y)
X_resampled.shape
y_resampled.shape
y_resampled['Downtime'].value_counts()

# split data
X_train, X_test, Y_train, Y_test = train_test_split(X_resampled, y_resampled, test_size = 0.2, random_state = 0, stratify = y_resampled)
# X_train, X_test, Y_train, Y_test = train_test_split(cleandata, df_y, test_size = 0.2, random_state = 0, stratify = df_y)

X_train.shape
X_test.shape
Y_train.value_counts()
Y_test.value_counts()


# Model building
# Best Model
############ Naive bayes from Automl Pycarret
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report

Gausian = GaussianNB(priors=None, var_smoothing=1e-09)

model_nb = Gausian.fit(X_train.values, Y_train.values.ravel())

pred = model_nb.predict(X_train.values)

accuracy = accuracy_score(Y_train, pred)
print("Test Accuracy:", accuracy)

report = classification_report(Y_train, pred)
print("Classification Report:\n", report)
####
pred_test = model_nb.predict(X_test.values)

# Evaluate the model's performance
accuracy = accuracy_score(Y_test, pred_test)
print("Test Accuracy:", accuracy)

report = classification_report(Y_test, pred_test)
print("Classification Report:\n", report)


pickle.dump(model_nb, open('Naive_bayes.pkl','wb'))

'''
# KNN -K Nearest Neighbours
from sklearn.neighbors import KNeighborsClassifier
import sklearn.metrics as skmet
knn = KNeighborsClassifier(n_neighbors = 21)

KNN = knn.fit(X_train.values, Y_train.values.ravel())  # Train the kNN model

# Explanation:
# .values will give the values in a numpy array (shape: (n,1))
# .ravel will convert that array shape to (n, ) (i.e. flatten it)

# Evaluate the model with train data
pred_train = knn.predict(X_train.values)  # Predict on train data

# Cross table
pd.crosstab(Y_train.values.ravel(), pred_train, rownames = ['Actual'], colnames = ['Predictions']) 

print(skmet.accuracy_score(Y_train, pred_train))  # Accuracy measure

# Predict the class on test data
pred = knn.predict(X_test.values)
pred

# Evaluate the model with test data
print(skmet.accuracy_score(Y_test, pred))
pd.crosstab(Y_test.values.ravel(), pred, rownames = ['Actual'], colnames = ['Predictions']) 

cm = skmet.confusion_matrix(Y_test, pred)

cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['Benign', 'Malignant'])
cmplot.plot()
cmplot.ax_.set(title = 'Cancer Detection - Confusion Matrix', 
               xlabel = 'Predicted Value', ylabel = 'Actual Value')

# creating empty list variable 
acc = []

# running KNN algorithm for 3 to 50 nearest neighbours(odd numbers) and 
# storing the accuracy values

for i in range(1, 50, 2):
    neigh = KNeighborsClassifier(n_neighbors = i)
    neigh.fit(X_train.values, Y_train.values.ravel())
    train_acc = np.mean(neigh.predict(X_train.values) == Y_train.values.ravel())
    test_acc = np.mean(neigh.predict(X_test.values) == Y_test.values.ravel())
    diff = train_acc - test_acc
    acc.append([diff, train_acc, test_acc])
    
acc
    
# Plotting the data accuracies
plt.plot(np.arange(1, 50, 2), [i[1] for i in acc], "ro-")
plt.plot(np.arange(1, 50, 2), [i[2] for i in acc], "bo-")


# Hyperparameter optimization
from sklearn.model_selection import GridSearchCV
knn = KNeighborsClassifier()

param_grid = {
    'n_neighbors': list(range(1, 50, 2)),    # Number of neighbors to consider
    'weights': ['uniform', 'distance'],    # Weight function used in prediction
}
  
# Defining parameter range
grid = GridSearchCV(knn, param_grid, cv = 5, 
                    scoring = 'accuracy', 
                    return_train_score = False, verbose = 1)

#help(GridSearchCV)

KNN_new = grid.fit(X_train.values, Y_train.values.ravel()) 

print(KNN_new.best_params_)

accuracy = KNN_new.best_score_ *100
print("Accuracy for our training dataset with tuning is : {:.2f}%".format(accuracy) )

# Predict the class on test data
pred = KNN_new.predict(X_test.values)
pred

print(skmet.accuracy_score(Y_test, pred))


cm = skmet.confusion_matrix(Y_test, pred)

cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['Benign', 'Malignant'])
cmplot.plot()
cmplot.ax_.set(title = 'Cancer Detection - Confusion Matrix', 
               xlabel = 'Predicted Value', ylabel = 'Actual Value')

from sklearn.tree import DecisionTreeClassifier
modelDT = DecisionTreeClassifier(criterion = 'entropy')

DT_model = modelDT.fit(X_train.values, Y_train.values.ravel())
from sklearn.metrics import accuracy_score, confusion_matrix

# Evaluation on Training Data
confusion_matrix(Y_train.values.ravel(), modelDT.predict(X_train.values))
accuracy_score(Y_train.values.ravel(), modelDT.predict(X_train.values))

# Evaluation on Testing Data
confusion_matrix(Y_test.values.ravel(), modelDT.predict(X_test.values))
accuracy_score(Y_test.values.ravel(), modelDT.predict(X_test.values))

# Hyperparameter Tuning for Decision Tree
# Create the Decision Tree Classifier
dt_classifier = DecisionTreeClassifier()
# Define the hyperparameters to tune and their possible values
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the GridSearchCV object
grid_search = GridSearchCV(dt_classifier, param_grid, cv=5)

# Fit the GridSearchCV object to the training data
grid_search.fit(X_train.values, Y_train.values.ravel())

# Print the best hyperparameters found by GridSearchCV
print("Best hyperparameters:", grid_search.best_params_)

# Get the best model
best_dt_model = grid_search.best_estimator_

# Evaluate the best model on the test set
test_accuracy = best_dt_model.score(X_test.values, Y_test.values.ravel())
print("Test accuracy with best model:", test_accuracy)

##################
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Create the Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Define the hyperparameters to tune and their possible values
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the GridSearchCV object
grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5)

# Fit the GridSearchCV object to the data
grid_search_rf.fit(X_train.values, Y_train.values.ravel())

# Get the best model
best_rf_model = grid_search_rf.best_estimator_

# Evaluate the best model on the test set
test_accuracy = best_rf_model.score(X_test.values, Y_test.values.ravel())
print("Test accuracy with best model:", test_accuracy)

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# Create the SVM Classifier

model_svc = SVC(kernel = 'rbf', gamma = 1)
model_svc.fit(X_train.values, Y_train.values.ravel())

train_accuracy = accuracy_score(Y_train.values.ravel(), model_svc.predict(X_train.values))
test_accuracy = accuracy_score(Y_test.values.ravel(), model_svc.predict(X_test.values))
train_accuracy, test_accuracy

## Hyperparameter Tuning
svm_classifier = SVC()
# Define the hyperparameters to tune and their possible values
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

# Create the GridSearchCV object
grid_search_svm = GridSearchCV(svm_classifier, param_grid_svm, cv=5)

# Fit the GridSearchCV object to the data
svm_grid = grid_search_svm.fit(X_train.values, Y_train.values.ravel())

# Get the best model
best_svm_model = grid_search_svm.best_estimator_

# Evaluate the best model on the test set
test_accuracy = best_svm_model.score(X_test.values, Y_test.values.ravel())
print("Test accuracy with best model:", test_accuracy)'''






